#!usr/bin/env python  
# -*- coding:utf-8 _*-  
""" 
@project:deepwalk-master
@author:xiangguosun 
@contact:sunxiangguodut@qq.com
@website:http://blog.csdn.net/github_36326955
@file: embed_nodes.py
@platform: macOS High Sierra 10.13.1 Pycharm pro 2017.1 
@time: 2018/09/13 
"""


import random
import os
import sys

from data.path import simple_walks as serialized_walks
from gensim.models import Word2Vec
import pickle
import torch
from collections import defaultdict
from pathlib import Path

# if __name__ == '__main__':

#     # æ£€æŸ¥ graph.nx æ˜¯å¦å­˜åœ¨
#     import os
#     base_dir = os.path.dirname(os.path.abspath(__file__))
#     graph_file = os.path.join(base_dir, '..', 'Amazon_Music', 'graph.nx')

#     print(f"graph.nx æ–‡ä»¶è·¯å¾„: {graph_file}")
#     print(f"æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(graph_file)}")
#     if os.path.exists(graph_file):
#         print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(graph_file)} å­—èŠ‚")

#     number_walks = 10
#     walk_length = 6 # length of path
#     workers = 2
#     representation_size = 100
#     window_size = 3
#     output = '../Amazon_Music/node.wv'

#     base_dir = Path(__file__).resolve().parent.parent  # ä¸Šä¸€çº§ç›®å½• data
#     graph_file = base_dir / 'Amazon_Music' / 'graph.nx'

#     G = pickle.load(open(graph_file, 'rb'))
#     # G = pickle.load(open('../Amazon_Music/graph.nx', 'rb')) #node åŒ…æ‹¬ user/item/brand/category/also_bought
#     walks_filebase = "../Amazon_Music/path/node_path/walks.txt"
#     nodewv = '../Amazon_Music/nodewv.dic'
#     print("Number of nodes: {}".format(G.number_of_nodes()))
#     print("Number of edges: {}".format(G.number_of_edges()))
#     print("number_walks: {}".format(number_walks))
#     num_walks = G.number_of_nodes() * number_walks
#     print("Number of walks: {}".format(num_walks))
#     data_size = num_walks * walk_length
#     print("Data size (walks*length): {}".format(data_size))

#     print(type(G))
#     Path("../Amazon_Music/path/node_path").mkdir(parents=True, exist_ok=True)

#     walk_files = serialized_walks.write_walks_to_disk(G, walks_filebase, num_paths=number_walks,
#                                                       path_length=walk_length, num_workers=workers, alpha=0.1,
#                                                       rand=random.Random(100), always_rebuild=True)  # , r=args.r)
#     # walk_files = ["../Amazon_Music/path/node_path/walks.txt.0", "../Amazon_Music/path/node_path/walks.txt.1"]
#     walks = serialized_walks.WalksCorpus(walk_files)


#     print("Training...")
#     # model = Word2Vec(walks, size=representation_size, window=window_size, min_count=0, sg=1, hs=1,
#     #                  workers=workers)

#     model = Word2Vec(
#         sentences=walks,
#         vector_size=representation_size,  # ä¹‹å‰çš„ size
#         window=window_size,
#         min_count=0,
#         sg=1,
#         hs=1,
#         workers=workers
#     )

#     model.wv.save_word2vec_format(output)

#     nodewv_dic = defaultdict(torch.Tensor)
#     with open(output, 'r') as f:
#         f.readline()
#         for line in f:
#             s = line.split()
#             nodeid = int(s[0])
#             fea = [float(x) for x in s[1:]]
#             nodewv_dic[nodeid] = torch.Tensor(fea)

#     pickle.dump(nodewv_dic, open(nodewv, 'wb'))
    
    

if __name__ == '__main__':
    # é…ç½®å‚æ•°
    number_walks = 10
    walk_length = 6
    workers = 2
    representation_size = 100
    window_size = 3
    
    # æ–‡ä»¶è·¯å¾„
    base_dir = Path(__file__).resolve().parent.parent  # ä¸Šä¸€çº§ç›®å½• data
    amazon_dir = base_dir / 'Amazon_Music'
    
    graph_file = amazon_dir / 'graph.nx'
    walks_filebase = amazon_dir / 'path' / 'node_path' / 'walks.txt'
    output_wv = amazon_dir / 'node.wv'
    nodewv_dic_file = amazon_dir / 'nodewv.dic'
    
    print("=" * 60)
    print("DeepWalk èŠ‚ç‚¹åµŒå…¥ç”Ÿæˆ")
    print("=" * 60)
    
    # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    print("\n1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶...")
    print(f"å›¾æ–‡ä»¶: {graph_file}")
    if not graph_file.exists():
        print(f"âŒ é”™è¯¯: å›¾æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
        print("è¯·å…ˆè¿è¡Œ data_process.py ç”Ÿæˆå›¾æ–‡ä»¶")
        sys.exit(1)
    
    # 2. åŠ è½½å›¾
    print("\n2. åŠ è½½å›¾...")
    try:
        G = pickle.load(open(graph_file, 'rb'))
        print(f"âœ… æˆåŠŸåŠ è½½å›¾")
        print(f"   èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
        print(f"   è¾¹æ•°: {G.number_of_edges()}")
        
        # æ£€æŸ¥å›¾æ˜¯å¦æ˜¯ç©ºçš„
        if G.number_of_edges() == 0:
            print("âš ï¸ è­¦å‘Š: å›¾ä¸­æ²¡æœ‰è¾¹ï¼Œéšæœºæ¸¸èµ°å¯èƒ½æ— æ³•è¿›è¡Œ")
            print("æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        print(f"âŒ åŠ è½½å›¾æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)
    
    # 3. åˆ›å»ºè¾“å‡ºç›®å½•
    print("\n3. å‡†å¤‡è¾“å‡ºç›®å½•...")
    walks_dir = amazon_dir / 'path' / 'node_path'
    walks_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… åˆ›å»ºç›®å½•: {walks_dir}")
    
    # 4. ç”Ÿæˆéšæœºæ¸¸èµ°
    print("\n4. ç”Ÿæˆéšæœºæ¸¸èµ°...")
    print(f"å‚æ•°: æ¯ä¸ªèŠ‚ç‚¹ {number_walks} æ¡æ¸¸èµ°, é•¿åº¦ {walk_length}")
    num_walks = G.number_of_nodes() * number_walks
    data_size = num_walks * walk_length
    print(f"æ€»æ¸¸èµ°æ•°: {num_walks}")
    print(f"æ€»æ•°æ®é‡: {data_size}")
    
    try:
        walk_files = serialized_walks.write_walks_to_disk(
            G, str(walks_filebase), 
            num_paths=number_walks,
            path_length=walk_length, 
            num_workers=workers, 
            alpha=0.1,
            rand=random.Random(100), 
            always_rebuild=True
        )
        print(f"âœ… æˆåŠŸç”Ÿæˆéšæœºæ¸¸èµ°æ–‡ä»¶")
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        if isinstance(walk_files, list):
            for wf in walk_files:
                if os.path.exists(wf):
                    size = os.path.getsize(wf)
                    print(f"   æ–‡ä»¶: {wf}, å¤§å°: {size} å­—èŠ‚")
        elif os.path.exists(walks_filebase):
            size = os.path.getsize(walks_filebase)
            print(f"   æ–‡ä»¶: {walks_filebase}, å¤§å°: {size} å­—èŠ‚")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆéšæœºæ¸¸èµ°å¤±è´¥: {e}")
        print("æ£€æŸ¥ simple_walks æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…")
        sys.exit(1)
    
    # 5. è®­ç»ƒ Word2Vec æ¨¡å‹
    print("\n5. è®­ç»ƒ Word2Vec æ¨¡å‹...")
    try:
        walks = serialized_walks.WalksCorpus(walk_files)
        print("âœ… æˆåŠŸåŠ è½½æ¸¸èµ°è¯­æ–™")
        
        model = Word2Vec(
            sentences=walks,
            vector_size=representation_size,
            window=window_size,
            min_count=0,
            sg=1,
            hs=1,
            workers=workers
        )
        print("âœ… æˆåŠŸè®­ç»ƒ Word2Vec æ¨¡å‹")
    except Exception as e:
        print(f"âŒ è®­ç»ƒ Word2Vec å¤±è´¥: {e}")
        sys.exit(1)
    
    # 6. ä¿å­˜æ¨¡å‹
    print("\n6. ä¿å­˜æ¨¡å‹...")
    try:
        model.wv.save_word2vec_format(str(output_wv))
        print(f"âœ… ä¿å­˜ Word2Vec æ ¼å¼åˆ°: {output_wv}")
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        nodewv_dic = defaultdict(torch.Tensor)
        with open(output_wv, 'r') as f:
            f.readline()  # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆå¤´ä¿¡æ¯ï¼‰
            for line in f:
                s = line.strip().split()
                if len(s) == representation_size + 1:
                    nodeid = int(s[0])
                    fea = [float(x) for x in s[1:]]
                    nodewv_dic[nodeid] = torch.Tensor(fea)
        
        pickle.dump(nodewv_dic, open(nodewv_dic_file, 'wb'))
        print(f"âœ… ä¿å­˜å­—å…¸æ ¼å¼åˆ°: {nodewv_dic_file}")
        print(f"   åµŒå…¥å‘é‡æ•°: {len(nodewv_dic)}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        sys.exit(1)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ DeepWalk åµŒå…¥ç”Ÿæˆå®Œæˆ!")
    print("=" * 60)
    
    # 7. éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
    print("\n7. éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:")
    generated_files = [
        (output_wv, "Word2Vecæ ¼å¼åµŒå…¥"),
        (nodewv_dic_file, "å­—å…¸æ ¼å¼åµŒå…¥"),
    ]
    
    for file_path, desc in generated_files:
        if file_path.exists():
            size = file_path.stat().st_size
            print(f"âœ… {desc}: {file_path}, å¤§å°: {size} å­—èŠ‚")
        else:
            print(f"âŒ {desc}: æ–‡ä»¶ä¸å­˜åœ¨")